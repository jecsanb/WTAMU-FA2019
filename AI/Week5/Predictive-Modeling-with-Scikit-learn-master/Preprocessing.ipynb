{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agenda**\n",
    "\n",
    "- What is preprocessing and how it is done?\n",
    "\n",
    "We will learn how to do the following:\n",
    "1. Train/Test split\n",
    "2. Normalization/Standardization\n",
    "3. Label Encoding\n",
    "4. Handling Missing Data\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing overview\n",
    "\n",
    "In simple words, pre-processing refers to the transformations applied to your data before feeding it to the algorithm \n",
    "\n",
    "Scikit-learn library has a pre-built functionality under **sklearn.preprocessing** that we will explore in this module\n",
    "\n",
    "## Train-Test split\n",
    "\n",
    "Setting portion of data aside - prerequisite for generalization\n",
    "\n",
    "Different partitions for training and testing - A core practice in machine learning\n",
    "\n",
    "Scikit-learn has a convenient method to assist in that process:\n",
    "\n",
    "**train_test_split(sample, response, test_size=0.25, shuffle=True)**\n",
    "\n",
    "The split size is controlled using the **attribute test_size** \n",
    "\n",
    "**Default test_size** - 25% of the dataset size\n",
    "\n",
    "**Standard practice** - shuffle the dataset before splitting by setting the attribute **shuffle=True**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# split in train and test sets\n",
    "iris = datasets.load_iris()\n",
    "iris.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, shuffle=True)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Scale or Not To Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling is a monotonic transformation \n",
    "* The relative order of smaller to larger value in a variable is maintained post the scaling\n",
    "\n",
    "Vast majority of algorithms require scaling\n",
    "\n",
    "Algorithms that do not require scaling:\n",
    "\n",
    "> Algorithms that rely on rules \n",
    "* They would not be affected by any monotonic transformations of the variables\n",
    "* e.g. CART, Random Forests, Gradient Boosted Decision Trees\n",
    "\n",
    "> Algorithms that rely on distributions of the variables\n",
    "* e.g. Naive Bayes\n",
    "\n",
    "[Standardization and Normalization](https://stats.stackexchange.com/questions/10289/whats-the-difference-between-normalization-and-standardization) are ways to do scaling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforms the features into a Standard Gaussian (or normal) distribution with a mean of 0 and standard deviation of 1\n",
    "\n",
    "Used when algorithm requires computation of distance (Euclidean) to avoid large scale features dominating others \n",
    "* e.g. KNN, K-means, Minimum distance classifier\n",
    "\n",
    "It matters in PCA to avoid bias towards high magnitude features \n",
    "\n",
    "For gradient descent based algorithm standardization helps in faster convergence\n",
    "\n",
    "In SVM it can reduce the time to find support vectors\n",
    "\n",
    "Scikit-learn implements data standardization in the StandardScaler module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "standardize_Xtrain = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardize_Xtrain[0:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Standardize X_test and print how first 5 rows of X_test and standardize_Xtest look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize X_test\n",
    "# print X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print Standardize_X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "Normalization transforms the features in the dataset so that it has a unit norm or has magnitude or length of 1 \n",
    "\n",
    "The length of a vector is the square-root of the sum of squares of the vector elements \n",
    "\n",
    "A unit vector (or unit norm) is obtained by dividing the vector by its length \n",
    "\n",
    "Note: Normalizing the dataset is particularly useful in scenarios where the dataset is sparse (i.e., a large number of observations are zeros) and also have differing scales \n",
    "\n",
    "Normalization in Scikit-learn is implemented in the Normalizer module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "scaler = Normalizer().fit(X_train)\n",
    "normalize_Xtrain = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_Xtrain[0:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Normalize X_test and print how first 5 rows of X_test and normalize_Xtest look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize X_test\n",
    "# print X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print Normalize_X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding or Encoding Categorical Variables\n",
    "\n",
    "Encoding - converting non-numerical features with labels into a numerical representation\n",
    "\n",
    "Encoding in Scikit-learn using LabelEncoder - for encoding labels as integers \n",
    "\n",
    "We have already seen how species were encoded in iris dataset:\n",
    "* 0 = setosa, 1 = versicolor, 2 = virginica\n",
    "\n",
    "In avocado dataset:\n",
    "The `region_categories` and `type_categories variables` store the unique categories in the `region` and `type` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "avocado_path = 'avocado.csv' #please make sure avacado.csv is in the same directory as the iPython notebook\n",
    "avocado_df = pd.read_csv(avocado_path,header=0)\n",
    "avocado_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "region_categories = avocado_df['region'].unique()\n",
    "type_categories = avocado_df['type'].unique()\n",
    "\n",
    "print('Region categories are: \\n',region_categories,'\\n')\n",
    "print('Type categories are: \\n',type_categories,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "region_encoder = LabelEncoder()\n",
    "encoded_region_cats = region_encoder.fit_transform(region_categories)\n",
    "print('The encoded region categories are:', encoded_region_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Before Encoding the region column was: \\n', avocado_df['region'])\n",
    "print('\\n')\n",
    "\n",
    "avocado_df['region'] = region_encoder.transform(avocado_df['region'])\n",
    "print('After Encoding the region column is: \\n', avocado_df['region'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Encode type categories print the encoded type categories and print the column before and after encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoded Type Categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the column before, encode type categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the column after encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Missing Data\n",
    "\n",
    "It is often the case that a dataset contains several missing observations \n",
    "\n",
    "Scikit-learn implements the Imputer module for completing missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will first make some holes in our data\n",
    "avocado_path = 'avocado.csv' #please make sure avacado.csv is in the same directory as the iPython notebook\n",
    "avocado_df = pd.read_csv(avocado_path,header=0)\n",
    "avocado_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "avocado_df.drop(avocado_df.columns[[0, 9, 10, 11, 12]], axis = 1, inplace = True) \n",
    "\n",
    "np.random.seed(100)\n",
    "mask = np.random.choice([True, False], size=avocado_df.shape)\n",
    "#print (mask)\n",
    "\n",
    "mask[mask.all(1),-1] = 0\n",
    "#print (mask)\n",
    "\n",
    "#print (avocado_df.mask(mask))\n",
    "\n",
    "#def add_random_na(row):\n",
    "#    vals = row.values\n",
    "#    for _ in range(random.randint(0,len(vals)-2)):\n",
    "#        i = random.randint(0,len(vals)-1)\n",
    "#        vals[i] = np.nan\n",
    "#    return vals\n",
    "\n",
    "avocado_df.mask(mask).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avocado_df.mask(mask).tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some versions of scikitlearn don't have module impute\n",
    "from sklearn.impute import SimpleImputer \n",
    "# from sklearn.preprocessing import Imputer\n",
    "\n",
    "# impute missing values - axix=0: impute along columns\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "# imputer = Imputer(missing_values=np.nan, strategy='mean')\n",
    "print(imputer.fit_transform(avocado_df.mask(mask)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
